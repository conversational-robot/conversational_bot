{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIm6Whe4RGmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "419263c5-8bc1-4cf7-b983-29366b32cab9"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten , Reshape\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('brown')\n",
        "data = brown.sents(categories=brown.categories())\n",
        "len(data)\n",
        "\n",
        "\n",
        "\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "punctuation = ['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','``',\"''\",'--']\n",
        "\n",
        "sentences=[]\n",
        "\n",
        "for sentence in data:\n",
        "    for word in stopwords_:\n",
        "        token=\" \"+word+\" \"\n",
        "        sentence=[item.replace(token,\" \") for item in sentence]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences[i]=[item.lower() for item in sentences[i]]\n",
        "    sentences[i]=[item*(len(item)>2) for item in sentences[i]]\n",
        "    for pun in punctuation:\n",
        "        sentences[i]=[item.replace(pun,\"\") for item in sentences[i]]\n",
        "        sentences[i]=[item for item in sentences[i] if item]\n",
        "print(sentences[100])\n",
        "\n",
        "\n",
        "\n",
        "print(len(sentences))\n",
        "print(sentences[0])\n",
        "print(sentences[:15])\n",
        "\n",
        "print(len(sentences))\n",
        "vocab_size=10000\n",
        "vector_dim=64\n",
        "maxlen=20\n",
        "window_size=3\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences[0])\n",
        "print(sentences[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "\n",
        "for sentence in sentences:\n",
        "  for i , context in enumerate(sentence):\n",
        "    for j in range(max(i - window_size , 0) , i):\n",
        "      x_train.append(word_index[sentence[i]])\n",
        "      y_train.append(word_index[sentence[j]])\n",
        "    for j in range(i+1 , min(i + window_size , len(sentence))):\n",
        "      x_train.append(word_index[sentence[i]])\n",
        "      y_train.append(word_index[sentence[j]])  \n",
        "\n",
        "  \n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size, vector_dim, input_length=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "model.fit(x_train , y_train , epochs=num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "['daniel', 'personally', 'led', 'the', 'fight', 'for', 'the', 'measure', 'which', 'had', 'watered', 'down', 'considerably', 'since', 'its', 'rejection', 'two', 'previous', 'legislatures', 'public', 'hearing', 'before', 'the', 'house', 'committee', 'revenue', 'and', 'taxation']\n",
            "57340\n",
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n",
            "[['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'that', 'any', 'irregularities', 'took', 'place'], ['the', 'jury', 'further', 'said', 'termend', 'presentments', 'that', 'the', 'city', 'executive', 'committee', 'which', 'had', 'overall', 'charge', 'the', 'election', 'deserves', 'the', 'praise', 'and', 'thanks', 'the', 'city', 'atlanta', 'for', 'the', 'manner', 'which', 'the', 'election', 'was', 'conducted'], ['the', 'septemberoctober', 'term', 'jury', 'had', 'been', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'the', 'hardfought', 'primary', 'which', 'was', 'won', 'mayornominate', 'ivan', 'allen', 'jr'], ['only', 'relative', 'handful', 'such', 'reports', 'was', 'received', 'the', 'jury', 'said', 'considering', 'the', 'widespread', 'interest', 'the', 'election', 'the', 'number', 'voters', 'and', 'the', 'size', 'this', 'city'], ['the', 'jury', 'said', 'did', 'find', 'that', 'many', 'georgias', 'registration', 'and', 'election', 'laws', 'are', 'outmoded', 'inadequate', 'and', 'often', 'ambiguous'], ['recommended', 'that', 'fulton', 'legislators', 'act', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'the', 'end', 'modernizing', 'and', 'improving', 'them'], ['the', 'grand', 'jury', 'commented', 'number', 'other', 'topics', 'among', 'them', 'the', 'atlanta', 'and', 'fulton', 'county', 'purchasing', 'departments', 'which', 'said', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'the', 'best', 'interest', 'both', 'governments'], ['merger', 'proposed'], ['however', 'the', 'jury', 'said', 'believes', 'these', 'two', 'offices', 'should', 'combined', 'achieve', 'greater', 'efficiency', 'and', 'reduce', 'the', 'cost', 'administration'], ['the', 'city', 'purchasing', 'department', 'the', 'jury', 'said', 'lacking', 'experienced', 'clerical', 'personnel', 'result', 'city', 'personnel', 'policies'], ['urged', 'that', 'the', 'city', 'take', 'steps', 'remedy', 'this', 'problem'], ['implementation', 'georgias', 'automobile', 'title', 'law', 'was', 'also', 'recommended', 'the', 'outgoing', 'jury'], ['urged', 'that', 'the', 'next', 'legislature', 'provide', 'enabling', 'funds', 'and', 'reset', 'the', 'effective', 'date', 'that', 'orderly', 'implementation', 'the', 'law', 'may', 'effected'], ['the', 'grand', 'jury', 'took', 'swipe', 'the', 'state', 'welfare', 'departments', 'handling', 'federal', 'funds', 'granted', 'for', 'child', 'welfare', 'services', 'foster', 'homes'], ['this', 'one', 'the', 'major', 'items', 'the', 'fulton', 'county', 'general', 'assistance', 'program', 'the', 'jury', 'said', 'but', 'the', 'state', 'welfare', 'department', 'has', 'seen', 'fit', 'distribute', 'these', 'funds', 'through', 'the', 'welfare', 'departments', 'all', 'the', 'counties', 'the', 'state', 'with', 'the', 'exception', 'fulton', 'county', 'which', 'receives', 'none', 'this', 'money']]\n",
            "57340\n",
            "[2, 5334, 609, 2220, 1569, 36, 1774, 2105, 1, 510, 1069, 1341, 1135, 432, 4, 54, 9049, 175, 135]\n",
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 1, 64)             640000    \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                4160      \n",
            "=================================================================\n",
            "Total params: 644,160\n",
            "Trainable params: 644,160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1AcpN4fRXQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "while(1):\n",
        "    a.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo-q42RoRZ4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = []\n",
        "WINDOW_SIZE = 2\n",
        "for sentence in sentences:\n",
        "    for word_index, word in enumerate(sentence):\n",
        "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
        "            if nb_word != word:\n",
        "                data.append([word, nb_word])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4p5lD8zUD9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "7f6aaf7c-76a5-4d57-c166-0ee55eaee43f"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLtmJDnvYapq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ol37K1Ynv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c6df763-6d4d-4f38-f204-6c0daea7c71e"
      },
      "source": [
        "vocab_size=len(sentences)\n",
        "words = []\n",
        "for i in range(len(sentences)):\n",
        "  words.append(sentences[i].split())\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(sentences))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbDUl1geYr98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences_padded = pad_sequences(sequences, padding='post', maxlen=max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3f4ZwZBc-GB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "da273ea4-abb7-42a8-c37d-2e115b1bec4e"
      },
      "source": [
        "x_train=[]\n",
        "y_train=[]\n",
        "for word in sequences:\n",
        "  x_train.append(sequences_padded[0])\n",
        "  y_train.append(sequences_padded[1])\n",
        "\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2225, 120)\n",
            "(2225, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDIVA_utdudj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "78cfd354-cb4d-4dfc-c048-9d50b10063a5"
      },
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten , Reshape\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "embedding_dimensions = 120\n",
        "model = Sequential()\n",
        "model.add(Embedding(120, embedding_dimensions, input_length=vocab_size))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 2225, 120)         14400     \n",
            "=================================================================\n",
            "Total params: 14,400\n",
            "Trainable params: 14,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUMwiJ8d4Pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "bb93966b-e0d6-4c08-c518-811b109c301d"
      },
      "source": [
        "batch_size = 50\n",
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=batch_size, \n",
        "                    epochs=1000,\n",
        "                    verbose=1)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-4e44cca50e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_13_input to have shape (2225,) but got array with shape (120,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8US4xOIeH-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}