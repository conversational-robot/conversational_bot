{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIm6Whe4RGmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "419263c5-8bc1-4cf7-b983-29366b32cab9"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten , Reshape\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('brown')\n",
        "data = brown.sents(categories=brown.categories())\n",
        "len(data)\n",
        "\n",
        "\n",
        "\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "punctuation = ['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','``',\"''\",'--']\n",
        "\n",
        "sentences=[]\n",
        "\n",
        "for sentence in data:\n",
        "    for word in stopwords_:\n",
        "        token=\" \"+word+\" \"\n",
        "        sentence=[item.replace(token,\" \") for item in sentence]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences[i]=[item.lower() for item in sentences[i]]\n",
        "    sentences[i]=[item*(len(item)>2) for item in sentences[i]]\n",
        "    for pun in punctuation:\n",
        "        sentences[i]=[item.replace(pun,\"\") for item in sentences[i]]\n",
        "        sentences[i]=[item for item in sentences[i] if item]\n",
        "print(sentences[100])\n",
        "\n",
        "\n",
        "\n",
        "print(len(sentences))\n",
        "print(sentences[0])\n",
        "print(sentences[:15])\n",
        "\n",
        "print(len(sentences))\n",
        "vocab_size=10000\n",
        "vector_dim=64\n",
        "maxlen=20\n",
        "window_size=3\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences[0])\n",
        "print(sentences[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "\n",
        "for sentence in sentences:\n",
        "  for i , context in enumerate(sentence):\n",
        "    for j in range(max(i - window_size , 0) , i):\n",
        "      x_train.append(word_index[sentence[i]])\n",
        "      y_train.append(word_index[sentence[j]])\n",
        "    for j in range(i+1 , min(i + window_size , len(sentence))):\n",
        "      x_train.append(word_index[sentence[i]])\n",
        "      y_train.append(word_index[sentence[j]])  \n",
        "\n",
        "  \n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size, vector_dim, input_length=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "model.fit(x_train , y_train , epochs=num_epochs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "['daniel', 'personally', 'led', 'the', 'fight', 'for', 'the', 'measure', 'which', 'had', 'watered', 'down', 'considerably', 'since', 'its', 'rejection', 'two', 'previous', 'legislatures', 'public', 'hearing', 'before', 'the', 'house', 'committee', 'revenue', 'and', 'taxation']\n",
            "57340\n",
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n",
            "[['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'that', 'any', 'irregularities', 'took', 'place'], ['the', 'jury', 'further', 'said', 'termend', 'presentments', 'that', 'the', 'city', 'executive', 'committee', 'which', 'had', 'overall', 'charge', 'the', 'election', 'deserves', 'the', 'praise', 'and', 'thanks', 'the', 'city', 'atlanta', 'for', 'the', 'manner', 'which', 'the', 'election', 'was', 'conducted'], ['the', 'septemberoctober', 'term', 'jury', 'had', 'been', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'the', 'hardfought', 'primary', 'which', 'was', 'won', 'mayornominate', 'ivan', 'allen', 'jr'], ['only', 'relative', 'handful', 'such', 'reports', 'was', 'received', 'the', 'jury', 'said', 'considering', 'the', 'widespread', 'interest', 'the', 'election', 'the', 'number', 'voters', 'and', 'the', 'size', 'this', 'city'], ['the', 'jury', 'said', 'did', 'find', 'that', 'many', 'georgias', 'registration', 'and', 'election', 'laws', 'are', 'outmoded', 'inadequate', 'and', 'often', 'ambiguous'], ['recommended', 'that', 'fulton', 'legislators', 'act', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'the', 'end', 'modernizing', 'and', 'improving', 'them'], ['the', 'grand', 'jury', 'commented', 'number', 'other', 'topics', 'among', 'them', 'the', 'atlanta', 'and', 'fulton', 'county', 'purchasing', 'departments', 'which', 'said', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'the', 'best', 'interest', 'both', 'governments'], ['merger', 'proposed'], ['however', 'the', 'jury', 'said', 'believes', 'these', 'two', 'offices', 'should', 'combined', 'achieve', 'greater', 'efficiency', 'and', 'reduce', 'the', 'cost', 'administration'], ['the', 'city', 'purchasing', 'department', 'the', 'jury', 'said', 'lacking', 'experienced', 'clerical', 'personnel', 'result', 'city', 'personnel', 'policies'], ['urged', 'that', 'the', 'city', 'take', 'steps', 'remedy', 'this', 'problem'], ['implementation', 'georgias', 'automobile', 'title', 'law', 'was', 'also', 'recommended', 'the', 'outgoing', 'jury'], ['urged', 'that', 'the', 'next', 'legislature', 'provide', 'enabling', 'funds', 'and', 'reset', 'the', 'effective', 'date', 'that', 'orderly', 'implementation', 'the', 'law', 'may', 'effected'], ['the', 'grand', 'jury', 'took', 'swipe', 'the', 'state', 'welfare', 'departments', 'handling', 'federal', 'funds', 'granted', 'for', 'child', 'welfare', 'services', 'foster', 'homes'], ['this', 'one', 'the', 'major', 'items', 'the', 'fulton', 'county', 'general', 'assistance', 'program', 'the', 'jury', 'said', 'but', 'the', 'state', 'welfare', 'department', 'has', 'seen', 'fit', 'distribute', 'these', 'funds', 'through', 'the', 'welfare', 'departments', 'all', 'the', 'counties', 'the', 'state', 'with', 'the', 'exception', 'fulton', 'county', 'which', 'receives', 'none', 'this', 'money']]\n",
            "57340\n",
            "[2, 5334, 609, 2220, 1569, 36, 1774, 2105, 1, 510, 1069, 1341, 1135, 432, 4, 54, 9049, 175, 135]\n",
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 1, 64)             640000    \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                4160      \n",
            "=================================================================\n",
            "Total params: 644,160\n",
            "Trainable params: 644,160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1AcpN4fRXQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo-q42RoRZ4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4p5lD8zUD9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLtmJDnvYapq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ol37K1Ynv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbDUl1geYr98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3f4ZwZBc-GB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDIVA_utdudj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUMwiJ8d4Pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8US4xOIeH-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}